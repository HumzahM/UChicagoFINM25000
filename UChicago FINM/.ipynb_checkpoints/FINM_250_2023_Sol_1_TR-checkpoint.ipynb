{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FINM 250 Homework 1\n",
    "## TA Solutions\n",
    "\n",
    "# Part 2\n",
    "\n",
    "### 1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Set the adjustment to be 12; in python, the convention\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# is to use all caps for global constants.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m ADJ \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m12\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the adjustment to be 12; in python, the convention\n",
    "# is to use all caps for global constants.\n",
    "ADJ = 12\n",
    "\n",
    "rets = pd.read_excel(\n",
    "    \"../data/multi_asset_etf_data.xlsx\",\n",
    "    sheet_name=\"excess returns\",  # Specify the sheet name\n",
    "    index_col=0,  # Set the index to the first column in the excel sheet\n",
    "    parse_dates=[\n",
    "        0\n",
    "    ],  # Parse the dates from the first column to turn them into datetime objects\n",
    ")\n",
    "\n",
    "# Sanity check to make sure the data is loaded correctly\n",
    "rets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dictionary to store the summary statistics, note that\n",
    "# in pandas you *generally* want to avoid building DataFrames iteratively,\n",
    "# ie. by appending rows and columns to a DataFrame one at a time, since\n",
    "# there is a big performance hit.\n",
    "\n",
    "\n",
    "def performance_summary(rets, adj_factor=12):\n",
    "    # Here, we use the keyword argument adj_factor to adjust the\n",
    "    # annualization factor, which, since in this course we will\n",
    "    # mainly deal with monthly data, is set to 12 by default.\n",
    "\n",
    "    summary = {}\n",
    "    summary[\"Annualized Mean\"] = rets.mean() * adj_factor\n",
    "    summary[\"Annualized Volatility\"] = rets.std() * np.sqrt(adj_factor)\n",
    "    summary[\"Annualized Sharpe Ratio\"] = (\n",
    "        summary[\"Annualized Mean\"] / summary[\"Annualized Volatility\"]\n",
    "    )\n",
    "    return pd.DataFrame(summary, index=rets.columns)\n",
    "\n",
    "\n",
    "# Since part b asks to find the best and worst Sharpe Ratios, we can\n",
    "# use the .sort_values method to sort the DataFrame by the Sharpe Ratio.\n",
    "metrics = performance_summary(rets).sort_values(\n",
    "    \"Annualized Sharpe Ratio\", ascending=False\n",
    ")\n",
    "metrics.style.format(\"{:.2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b\n",
    "\n",
    "We see that SPY has the best Sharpe ratio, with a value of 0.96. This is somewhat expected, given the incredible bull-run that SPY had from 2009-2022, and then again from late 2022 to 2023. HYG (high-yield corporate bonds) has the second best, at 0.71. BWX has the worst Sharpe ratio, at -0.0022, and is also the only ETF with negative returns over the period. Second worst is DBC, with a Sharpe ratio of 0.11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = rets.corr()\n",
    "\n",
    "# Plot the correlation matrix using seaborn's heatmap function, there are a lot of arguments here,\n",
    "# so do not worry about understanding them, simply calling sns.heatmap() with the corr_matrix\n",
    "# is enough to get the job done.\n",
    "\n",
    "\n",
    "def plot_corr_matrix(corrs):\n",
    "    return sns.heatmap(\n",
    "        corrs,  # The correlation matrix\n",
    "        annot=True,  # Annot means include the correlation values in the heatmap\n",
    "        cmap=\"coolwarm\",  # The color scheme to use\n",
    "        vmin=-1,\n",
    "        vmax=1,  # Specify the max and min values for the color scheme, otherwise they will\n",
    "        # be set to the min and max values of the correlation matrix, which is not\n",
    "        # particularly useful.\n",
    "        linewidths=0.7,  # The width of the lines that will divide each cell, mainly an aesthetic choice,\n",
    "        # but it does make the heatmap easier to read.\n",
    "        annot_kws={\n",
    "            \"size\": 10\n",
    "        },  # Specify the size of the annotation text to avoid overflow into adjacent cells.\n",
    "        fmt=\".2f\",  # Specify the format of the annotation text, in this case, we want to round to 2 decimal places.\n",
    "        # See: https://kuvapcsitrd01.kutztown.edu/~schwesin/fall20/csc223/lectures/Python_String_Formatting.html\n",
    "    )\n",
    "\n",
    "\n",
    "fig, _ = plt.subplots(figsize=(6, 5))\n",
    "ax = plot_corr_matrix(corr_matrix)\n",
    "\n",
    "# Specify the axis to plot on, this is NOT necessary, but it is good when using multiple subplots.\n",
    "ax.set_title(\"Correlation Matrix\")  # Set the title of the plot\n",
    "fig.tight_layout()  # This adjusts spacing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get highest and lowest correlations.\n",
    "# Unstack the correlation matrix to get a series of all the correlations.\n",
    "corr_series = corr_matrix.unstack()\n",
    "\n",
    "# Next, we want to remove all the correlations of each asset with itself, since\n",
    "# they will all be 1, and we are not interested in them.\n",
    "corr_series = corr_series[corr_series != 1]\n",
    "max_corr = corr_series.idxmax()\n",
    "min_corr = corr_series.idxmin()\n",
    "\n",
    "# Get the values.\n",
    "max_corr_val = corr_series[max_corr]\n",
    "min_corr_val = corr_series[min_corr]\n",
    "\n",
    "# Note, the above code can also be written by using the DataFrame.agg method, which\n",
    "# allows us to apply multiple functions to a DataFrame at once.\n",
    "max_corr = corr_series.abs().agg([\"idxmax\", \"max\"]).T\n",
    "min_corr = corr_series.abs().agg([\"idxmin\", \"min\"]).T\n",
    "min_corr_raw = corr_series.agg([\"idxmin\", \"min\"]).T\n",
    "max_corr, max_corr_val = max_corr[\"idxmax\"], max_corr[\"max\"]\n",
    "min_corr, min_corr_val = min_corr[\"idxmin\"], min_corr[\"min\"]\n",
    "min_corr_raw, min_corr_raw_val = min_corr_raw[\"idxmin\"], min_corr_raw[\"min\"]\n",
    "\n",
    "# Print the results. Note how we use {:.2f} to round to 2 decimal places.\n",
    "print(\n",
    "    f\"Max Corr (by absolute value): {max_corr[0]} and {max_corr[1]} with a correlation of {max_corr_val:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Min Corr (by absolute value): {min_corr[0]} and {min_corr[1]} with a correlation of {min_corr_val:.2f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Min Corr (raw): {min_corr_raw[0]} and {min_corr_raw[1]} with a correlation of {min_corr_raw_val:.2f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the assets have quite high correlations with each other, as shown by the heatmap. In fact, the only asset that displays negative correlations with the others is IEF. TIP is also on the lower end of the correlation spectrum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is quite an open ended question, since what really defines \"outperformance\" is\n",
    "# somewhat subjective. We can look at the mean returns and sharpe ratio. You'll be better\n",
    "# able to answer this question after we cover risk metrics.\n",
    "#\n",
    "# Domestic bond ETF is IEF, foreign is BWX.\n",
    "\n",
    "# Get the mean returns and sharpe ratios.\n",
    "bonds = metrics.loc[[\"TIP\", \"IEF\", \"BWX\"]]\n",
    "\n",
    "# Get the rank of the metrics.\n",
    "bonds.loc[:, [\"Rank Mean\", \"Rank Vol\", \"Rank Sharpe\"]] = (\n",
    "    metrics.rank(ascending=False)\n",
    "    .loc[\n",
    "        [\"TIP\", \"IEF\", \"BWX\"],\n",
    "        [\"Annualized Mean\", \"Annualized Volatility\", \"Annualized Sharpe Ratio\"],\n",
    "    ]\n",
    "    .values\n",
    ")\n",
    "\n",
    "bonds.style.format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that TIP performs in about the middle of the pack, coming in 6th out of 11 in terms of Sharpe, and 7th out of 11 in terms of mean. However, it does have the second lowest volatility, which is a good thing.\n",
    "\n",
    "I would argue that it outperforms both IEF and BWX. TIP has both a higher mean and lower volatility than the other two ETFs. So, yes, they have outperformed the other types of bonds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the correlations between the assets.\n",
    "corr_matrix.loc[[\"TIP\", \"IEF\", \"BWX\"], [\"TIP\", \"IEF\", \"BWX\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.c\n",
    "\n",
    "The answer for this is \"it depends\". Although TIP perform better than the other two bond ETFs, they have a high correlation to both. So, in that sense they do expand the investment opportunity set. However, IEF is just 7-10 year treasuries, and thus is not meaningfully different from TIP, which are inflation-protected treasuries. Therefore, I would argue that Harvard should not consider them as their own asset class, but rather incorporate them into their existing treasury portfolio.\n",
    "\n",
    "However, if we are limited to these choices of ETFs, rather than Harvard's thousands of assets, then yes, we should consider them their own asset class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tan_portfolio(mean_rets, cov_matrix):\n",
    "    \"\"\"\n",
    "    Function to calculate tangency portfolio weights. Comes from the\n",
    "    formula seen in class (Week 1).\n",
    "\n",
    "    Args:\n",
    "        mean_rets: Vector of mean returns.\n",
    "        cov_matrix: Covariance matrix of returns.\n",
    "\n",
    "    Returns:\n",
    "        Vector of tangency portfolio weights.\n",
    "    \"\"\"\n",
    "    inv_cov = np.linalg.inv(cov_matrix)\n",
    "    ones = np.ones(mean_rets.shape)\n",
    "    return (inv_cov @ mean_rets) / (ones.T @ inv_cov @ mean_rets)\n",
    "\n",
    "\n",
    "def gmv_portfolio(cov_matrix):\n",
    "    \"\"\"\n",
    "    Function to calculate the weights of the global minimum variance portfolio.\n",
    "\n",
    "    Args:\n",
    "        cov_matrix : Covariance matrix of returns.\n",
    "\n",
    "    Returns:\n",
    "        Vector of GMV portfolio weights.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        cov_inv = np.linalg.inv(cov_matrix)\n",
    "    except TypeError:\n",
    "        cov_inv = np.linalg.inv(np.array(cov_matrix))\n",
    "\n",
    "    one_vector = np.ones(len(cov_matrix.index))\n",
    "    return cov_inv @ one_vector / (one_vector @ cov_inv @ (one_vector))\n",
    "\n",
    "\n",
    "def mv_portfolio(mean_rets, cov_matrix, target=None):\n",
    "    \"\"\"\n",
    "    Function to calculate the weights of the mean-variance portfolio. If\n",
    "    target is not specified, then the function will return the tangency portfolio.\n",
    "    If target is specified, then we return the MV-efficient portfolio with the target\n",
    "    return.\n",
    "\n",
    "    Args:\n",
    "        mean_rets : Vector of mean returns.\n",
    "        cov_matrix : Covariance matrix of returns.\n",
    "        target (optional):  Target mean return. Defaults to None. Note: must be adjusted for\n",
    "                            annualization the same time-frequency as the mean returns. If the\n",
    "                            mean returns are monthly, the target must be monthly as well.\n",
    "\n",
    "    Returns:\n",
    "        Vector of MV portfolio weights.\n",
    "    \"\"\"\n",
    "    w_tan = tan_portfolio(mean_rets, cov_matrix)\n",
    "\n",
    "    if target is None:\n",
    "        return w_tan\n",
    "\n",
    "    w_gmv = gmv_portfolio(cov_matrix)\n",
    "    delta = (target - mean_rets @ w_gmv) / (mean_rets @ w_tan - mean_rets @ w_gmv)\n",
    "    return delta * w_tan + (1 - delta) * w_gmv\n",
    "\n",
    "\n",
    "# Note: we are NOT annualizing here.\n",
    "w_tan = mv_portfolio(rets.mean(), rets.cov())\n",
    "\n",
    "w_tan_df = pd.DataFrame(w_tan, index=rets.columns, columns=[\"Tangency Portfolio\"])\n",
    "\n",
    "# Here, we use the display() function to show both the weights and the performance summary,\n",
    "# without having to make two separate cells, and not mess up the formatting caused by print().\n",
    "display(\n",
    "    w_tan_df.sort_values(by=\"Tangency Portfolio\", ascending=False).style.format(\n",
    "        \"{:.2f}\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Calculate the portfolio returns.\n",
    "w_tan_rets = rets @ w_tan_df\n",
    "\n",
    "# We could use linear algebra to directly calculate the\n",
    "# portfolio statistics, but we will stick to using the\n",
    "# performance_summary function for consistency, and also,\n",
    "# if we want to calculate other risk metrics, we will need\n",
    "# to use the performance_summary function.\n",
    "tan_summ = performance_summary(w_tan_rets)\n",
    "display(tan_summ.style.format(\"{:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_port = pd.DataFrame(\n",
    "    mv_portfolio(rets.mean(), rets.cov(), target=0.0075),\n",
    "    index=rets.columns,\n",
    "    columns=[\"Target Portfolio\"],\n",
    ")\n",
    "target_rets = rets @ target_port\n",
    "target_summ = performance_summary(target_rets)\n",
    "\n",
    "# Note: I'm sorting by target portfolio here,\n",
    "# and not the tangency portfolio like in question 3.\n",
    "display(\n",
    "    pd.concat([w_tan_df, target_port, metrics[[\"Annualized Sharpe Ratio\"]]], axis=1)\n",
    "    .sort_values(by=\"Target Portfolio\", ascending=False)\n",
    "    .style.format(\"{:.3f}\")\n",
    ")\n",
    "display(pd.concat([tan_summ, target_summ], axis=0).style.format(\"{:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.c\n",
    "\n",
    "The portfolio is most long in IEF, followed by SPY and HYG. It is the most short in BWX (by quite a large margin), followed by PSP and QAI. Note that the target portfolio has the same top 3 and bottom 3 weights as the tangency, just with a different ordering. \n",
    "\n",
    "### 4.d\n",
    "\n",
    "Yes and no. SPY has the best Sharpe ratio, and we are long SPY. BWX has the worst Sharpe ratio and we are short BWX. However, we are short IYR, which has the 3rd highest Sharpe ratio. So, overall, I would argue that for the most long and most short allocations, it roughly aligns with the Sharpe ratios of the assets, but there is no clear pattern. This makes sense given that the it's the *covariance* between assets that matters the most, not just the returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the equal weighted portfolio.\n",
    "w_eq = np.ones(len(rets.columns)) / len(rets.columns)\n",
    "\n",
    "# Rescale to fit to target return of 0.0075.\n",
    "w_eq = w_eq * (0.0075 / (rets.mean() @ w_eq))\n",
    "\n",
    "# Calculate the returns of the equal weighted portfolio.\n",
    "w_eq_rets = (rets @ w_eq).to_frame(\"Equally Weighted\")\n",
    "\n",
    "# Risk parity portfolio.\n",
    "w_rp = np.array(1 / np.sqrt(np.diag(rets.cov())))\n",
    "\n",
    "# Again, rescale to fit to target return of 0.0075.\n",
    "w_rp = w_rp * (0.0075 / (rets.mean() @ w_rp))\n",
    "\n",
    "# Calculate the returns of the risk parity portfolio.\n",
    "w_rp_rets = (rets @ w_rp).to_frame(\"Risk Parity\")\n",
    "\n",
    "# Calculate performance summaries.\n",
    "w_eq_summ = performance_summary(w_eq_rets)\n",
    "w_rp_summ = performance_summary(w_rp_rets)\n",
    "\n",
    "w_comp = pd.DataFrame(\n",
    "    np.array([w_eq, w_rp]).T,\n",
    "    index=rets.columns,\n",
    "    columns=[\"Equally Weighted\", \"Risk Parity\"],\n",
    ")\n",
    "\n",
    "w_comp[\"Tangency\"] = w_tan_df\n",
    "w_comp[\"Target\"] = target_port\n",
    "\n",
    "# Concat and display.\n",
    "display(w_comp.style.format(\"{:.3f}\"))\n",
    "pd.concat([w_eq_summ, w_rp_summ, tan_summ, target_summ], axis=0).T.style.format(\n",
    "    \"{:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.c\n",
    "\n",
    "It's very different. For starters, both the tangency and the target portfolio have long/short positions. The equally weighted and risk parity ones don't. Moreover, all of the weights are quite a bit smaller in the equally weighted and risk parity portfolios. The tangency and target portfolios are much more concentrated in their top 3 and bottom 3 assets. This also makes sense since by shorting assets, you can have much bigger magnitudes of weights (that still sum to 1) than if you only go long.\n",
    "\n",
    "Both portfolios have worse Sharpe ratios than the tangency and target MV portfolio. This is to be expected given that by definition the tangency maximizes sharpe, and the target portfolio maximizes sharpe subject to a target return. The equally weighted weighted and risk parity portfolios have much higher volatility than the target MV portfolio. They also have very similar Sharpe ratios and volatilities.\n",
    "\n",
    "This is somewhat unexpected given that by the risk-parity definition, the weight of an asset is inversely proportional to the volatility, so I would have expected it to have lower volatilility. However, this makes sense if we consider that the main factor is the covariances, which risk-parity does not account for at all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop TIPS.\n",
    "rets_no_tips = rets.drop(\"TIP\", axis=1)\n",
    "\n",
    "# Calculate the tangency portfolio.\n",
    "w_tan_no_tips = mv_portfolio(rets_no_tips.mean(), rets_no_tips.cov())\n",
    "\n",
    "# Calculate the returns of the tangency portfolio.\n",
    "w_tan_no_tips_rets = (rets_no_tips @ w_tan_no_tips).to_frame(\n",
    "    \"Tangency Portfolio (No TIPS)\"\n",
    ")\n",
    "\n",
    "# Calculate the performance summary.\n",
    "w_tan_no_tips_summ = performance_summary(w_tan_no_tips_rets)\n",
    "\n",
    "# Display weights\n",
    "display(\n",
    "    pd.concat(\n",
    "        [\n",
    "            w_tan_df,\n",
    "            pd.Series(\n",
    "                w_tan_no_tips,\n",
    "                name=\"Tangency Portfolio (No TIPs)\",\n",
    "                index=rets_no_tips.columns,\n",
    "            ),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    .sort_values(by=\"Tangency Portfolio\", ascending=False)\n",
    "    .style.format(\"{:.2f}\")\n",
    ")\n",
    "\n",
    "pd.concat([w_tan_no_tips_summ, tan_summ], axis=0).T.style.format(\"{:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It, of course, has a lower Sharpe (but only by a little bit -- 1.718 vs 1.720). The reason for this is that adding assets **cannot** decrease the Sharpe ratio of the tangency portfolio. This is because if an asset contributes nothing, it will just have 0 weight in the tangency, so the best we can do is maintain an equal portfolio when we remove an asset. However, if the asset that we remove has non-zero weight, then it must decrease the Sharpe of the tangency.\n",
    "\n",
    "What is interesting is how much higher the mean and vol are of this new portfolio. Additionally, whilst the weights agree in terms of which are long/short, the magnitudes are vastly different. For example, QAI went from a weight of -50 to -77, and SPY/IEF went from mid-20s to low-40s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_tan_notips_target = mv_portfolio(\n",
    "    rets_no_tips.mean(), rets_no_tips.cov(), target=0.0075\n",
    ")\n",
    "\n",
    "w_tan_notips_target_rets = (rets_no_tips @ w_tan_notips_target).to_frame(\n",
    "    \"Target Portfolio (No TIPS)\"\n",
    ")\n",
    "\n",
    "w_tan_notips_target_summ = performance_summary(w_tan_notips_target_rets).T\n",
    "w_tan_notips_target_summ[\"Target Portfolio\"] = target_summ.T[\"Target Portfolio\"]\n",
    "\n",
    "target_port.loc[:\"SPY\", \"Target Portfolio (No TIPS)\"] = w_tan_notips_target\n",
    "display(target_port[target_port.columns[::-1]].style.format(\"{:.3f}\"))\n",
    "display(w_tan_notips_target_summ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has extremely similar allocations across the board, all positions where the original target portfolio was short are still short, all positions where the target portfolio was long are still long. The only big-ish difference is that the IEF allocation dropped from 0.844 to 0.708. This intuitively makes sense, given that TIP had the highest correlation with IEF, and thus we've essentially transfered the weight that was on TIP to IEF.\n",
    "\n",
    "The Sharpe ratios and volatilities are almost identical, meaning that we haven't lost much by discarding TIPS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3 - Extras\n",
    "\n",
    "### 1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_asset = rets[[\"TIP\", \"IEF\"]]\n",
    "\n",
    "w_two_asset = mv_portfolio(two_asset.mean(), two_asset.cov(), target=0.0135)\n",
    "w_two_asset = pd.DataFrame(w_two_asset, index=two_asset.columns, columns=[\"Two Asset\"])\n",
    "display(w_two_asset.style.format(\"{:.2f}\"))\n",
    "display(performance_summary(two_asset @ w_two_asset).style.format(\"{:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b\n",
    "\n",
    "The cause behind the extreme long/short is that in the two asset case, we short the asset with the lower Sharpe, to fund the asset with the higher Sharpe. Thus, we are going \"all-in\" on the asset with the higher Sharpe.\n",
    "\n",
    "### 1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_asset_perturbed = two_asset.mean()\n",
    "two_asset_perturbed[\"TIP\"] -= 0.0015\n",
    "\n",
    "w_two_asset_perturbed = mv_portfolio(\n",
    "    two_asset_perturbed, two_asset.cov(), target=0.0135\n",
    ")\n",
    "\n",
    "w_two_asset_perturbed = pd.DataFrame(\n",
    "    w_two_asset_perturbed, index=two_asset.columns, columns=[\"Two Asset Perturbed\"]\n",
    ")\n",
    "\n",
    "display(w_two_asset_perturbed.style.format(\"{:.2f}\"))\n",
    "display(performance_summary(two_asset @ w_two_asset_perturbed).style.format(\"{:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We entirely flipped the allocation, and now have negative returns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d\n",
    "\n",
    "It tells you that they are not precise whatsoever (vanilla MV). This is because due to high correlations, the covariance matrix is close to singular, and thus the inverse is very unstable. That means that even a small (1.5 basis-point) change in the returns can cause a huge change in the weights.\n",
    "\n",
    "### 2.a, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_diag = np.diag(np.diag(rets.cov()))\n",
    "\n",
    "cov_diag = pd.DataFrame(cov_diag, index=rets.columns, columns=rets.columns)\n",
    "\n",
    "w_diag = mv_portfolio(rets.mean(), cov_diag, target=0.0075)\n",
    "w_diag = pd.DataFrame(w_diag, index=rets.columns, columns=[\"Diagonal Covariance\"])\n",
    "\n",
    "w_diag_rets = rets @ w_diag\n",
    "\n",
    "# Compare with risk parity.\n",
    "w_diag[\"Risk Parity\"] = w_rp\n",
    "w_diag[\"Target Portfolio\"] = target_port[\"Target Portfolio\"]\n",
    "w_diag_summ = performance_summary(w_diag_rets).T\n",
    "w_diag_summ[\"Risk Parity\"] = w_rp_summ.T\n",
    "w_diag_summ[\"Target Portfolio\"] = target_summ.T[\"Target Portfolio\"]\n",
    "\n",
    "display(w_diag.style.format(\"{:.2f}\"))\n",
    "display(w_diag_summ.style.format(\"{:.2f}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.a - Discussion\n",
    "\n",
    "It looks quite different from the MV target portfolio, but somewhat similar to the risk-parity one. It has a better Sharpe ratio than the risk-parity portfolio, whilst also not have the extreme weights that the MV target portfolio has.\n",
    "\n",
    "### 2.b\n",
    "\n",
    "Between 3.1 and 3.2, this tells us that the tangency portfolio is highly unstable. Thus, it can be quite hard to implement it in practice, since we expect to have quite a bit of error in our estimates of the returns and covariances.\n",
    "\n",
    "### 2.c\n",
    "\n",
    "The pros: you are able to keep the weights in-line by using simple heuristics which place bounds on the weights. You can then use MV-optimization to get the best weights within those bounds. It's also easy to interpret, and easy to implement. Moreover, you can follow the mandate of the fund, and draw on the experience of portfolio managers when imposing those constraints. \n",
    "\n",
    "Also, using a diagonalized covariance matrix is somewhat suspect, given that we **know** assets have non-zero covariances so it is an extremely strong assumption to make. Beyond this course: we can also use more advanced shrinkage methods such as Ledoit-Wolf, OAS, or Marcenko-Pastur.\n",
    "\n",
    "The cons: you can needlessly limit your Sharpe ratio if one of the constraints you place is bounding. By removing the constraints you're able to get a purely quantitative allocation, which likely will perform better than the constrained one. It is also somewhat arbitrary, and can be hard to justify why you chose the constraints that you did.\n",
    "\n",
    "### 2.d\n",
    "\n",
    "It agrees with risk-parity for the most part. The weights are mostly the same, with the exception of BWX, where we are short in the diagonalized case, but long in the risk-parity case. This makes sense since BWX has low variance but also low returns, and risk-parity only looks at the variances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.a,b,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You may cover iterators with Seb later in the course,\n",
    "# but itertools (and the external library more_itertools)\n",
    "# are among the most useful libraries in Python when it comes\n",
    "# using any kind of looping.\n",
    "from itertools import pairwise\n",
    "\n",
    "summary_comp = pd.DataFrame(\n",
    "    columns=[\"Equal Weight\", \"Risk Parity\", \"Tangency\", \"Diagonal\"]\n",
    ")\n",
    "\n",
    "# Step through 2014-2023...\n",
    "for curr_year, next_year in pairwise(range(2014, 2023)):\n",
    "    # Turn year to string.\n",
    "    curr_year = str(curr_year)\n",
    "    next_year = str(next_year)\n",
    "\n",
    "    # Get returns for this year and next year\n",
    "    in_sample = rets.loc[curr_year, :]\n",
    "    out_sample = rets.loc[next_year, :]\n",
    "\n",
    "    # Calculate the covariance matrix and diagonalized covariance matrix.\n",
    "    is_cov = in_sample.cov()\n",
    "    is_cov_diag = pd.DataFrame(\n",
    "        np.diag(np.diag(is_cov)), index=in_sample.columns, columns=in_sample.columns\n",
    "    )\n",
    "    is_mean = in_sample.mean()\n",
    "\n",
    "    # Calculate the different weights & Returns\n",
    "    is_w_eq = np.ones(len(in_sample.columns)) / len(in_sample.columns)\n",
    "    w_eq_rets = (out_sample @ is_w_eq).to_frame(\"Equal Weight\")\n",
    "\n",
    "    is_w_rp = np.array(1 / np.sqrt(np.diag(is_cov)))\n",
    "    w_rp_rets = (out_sample @ is_w_rp).to_frame(\"Risk Parity\")\n",
    "\n",
    "    is_w_tan = mv_portfolio(is_mean, is_cov)\n",
    "    is_w_diag = mv_portfolio(is_mean, is_cov_diag)\n",
    "\n",
    "    w_tan_rets = (out_sample @ is_w_tan).to_frame(\"Tangency\")\n",
    "    w_diag_rets = (out_sample @ is_w_diag).to_frame(\"Diagonal\")\n",
    "\n",
    "    # Append to existing...\n",
    "    all_rets = pd.concat([w_eq_rets, w_rp_rets, w_tan_rets, w_diag_rets], axis=1)\n",
    "    summary_comp = pd.concat([summary_comp, all_rets], axis=0)\n",
    "\n",
    "performance_summary(summary_comp).style.format(\"{:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Risk-parity does the worst, and also has the highest volatility. Equally weighted performs the best, with tangency and diagonalized covariance close behind. There is something to be said for the simplicity of equally weighted, and it's ability to perform well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
